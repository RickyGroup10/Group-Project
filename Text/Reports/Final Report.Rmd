---
title: "A look at dating app user profile statistics"
author: "Lissa Harrop, Katrina Watkins, Ricky Loo and Max Tan"
date: "October 2022"
classoption: 
  - 12pt
  
header-includes:
  - \usepackage{newpxtext,eulerpx}
  - \usepackage{bm,bbm}
  - \usepackage{float}
  - \floatplacement{figure}{H}
  - \usepackage{titling}
  - \pretitle{\begin{center}
    \includegraphics[width=2in]{"../../Images/PNG/Hearts_final.png"}\LARGE\\}
  - \posttitle{\end{center}}

output: bookdown::pdf_document2

link-citations: yes
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(300521444, kind="Mersenne-Twister") #Mixture of all our ID numbers
require(ggplot2)
require(ggthemes)
require(extrafont)
require(booktabs)
require(GGally)
require(ggExtra)
require(xtable)
require(moments)
require(ggcorrplot)
require(fitdistrplus)
require(reshape2)
require(ggpubr)
require(dplyr)
require(caret)
require(stringr)
require(ggord)
require(HDtest)
require(MASS)
require(klaR)
require(factoextra)
require(psych)
require(nnet)
```

# Introduction

This study looks at Dating App User Profiles' stats from the Lovoo dating app. The dataset is available on kaggle (@Lovoov3) and the license to use the dataset is available on creativecommons (@License).

Lovoo, is a European dating platform that allows individuals to chat to each other before a mutuality of attraction, unlike other dating platforms that emphasize this before allowing any two people to get in touch and chat. The Lovoo dataset contains profile information on 3992 females that was obtained during April and May of 2015. The dataset contains 22 numerical and 20 categorical variables. We found that there was no information provided about what some of the variables mean. Several of the variables are unique identifiers such as name, userId and pictureId, among others. There are also many variables that are binary classifiers.

This study focuses on seven variables: age, counts_pictures, counts_profileVisits, counts_kisses, distance, country and isVip. Where **Age** is the users age, **counts_pictures** is the number of pictures on the user's profile, **counts_profileVisits** is the number of clicks on this user (to see his/her full profile) from other user accounts, **counts_kisses** is the number of unique user accounts that "liked" (called "kiss" on the platform) this user account, **distance** is the distance between this user's city/location and the location of the user account that was used to fetch the data of this user, **country** is the user's country, **isVip** is a 0 if the user is not a VIP and is 1 if the user is VIP. It was possible to buy a VIP status with real money. This status came with benefits.

Based on the features we have available, we would like to know what combination of these makes a person more charismatic and worthy of ‘love’ (or rather a kiss on their profile). We are hoping this will teach us how many photos we should include in our Lovoo profiles, or if we should move to Switzerland for a better chance at a successful dating outcome.

# Overall Methodology

During our exploratory data analysis it was discovered that there were 46 missing values in the variable distance. These have been replaced by the mean of the distance column (207.23). This ensured that we had a full dataset with a sample size of 3992 for all seven variables.

It was also discovered that our data were not normally distributed (and verified by Shapiro-Wilk normality tests), were heavily skewed and had several outliers that were influencing our results. As a result of this we chose to standardise our numerical variables and remove the outliers. We standardised the data through z-score standardisation and removed the outliers by removing any rows of data that had a value greater than 3 (this is three standard deviations). As a result of this our dataset was reduced to a sample size of 3816 users. 

For the categorical variable country we saw that 28 of the 32 countries have 20 or less users with 13 of these having only 1 user. As a result we decided to group all countries that had 20 users or less into one new category called “other”. This gave us 5 countries – Italy, France, Germany, Switzerland and Other.

We will use cluster, linear discriminant, and principal component analysis to answer our question. We have decided to use these techniques as we believe that each will contribute to our investigation of the “perfect charismatic dating profile”. Clustering will alert us to what differentiates unknown groups, whereas LDA and PCA will help us to examine whether we are combining the “right” predictors to find our perfect someone.

# Exploratory data anaylsis

## Results

The exploratory data analysis (EDA) started with a univariate analysis of our seven variables. 

Looking at the variable age we discovered that the median age of a user on the Lovoo dating app is 22 years old, with the youngest users being 18 years old and the oldest users being 28 years old. With approximately 75% of users being aged 20 to 24 years.

The median number of pictures a user has on their profile is 4, however there are some users that have no pictures and others that have up to 30 pictures. The bulk of users tend to have less than 10 profile pictures on their account, with a small number of users having between 11 and 30 profile pictures.

The median number of clicks on a user’s profile to see his/her full profile (from another user's account) is 1222, with some users having no visits to their profile and some having up to 164425 visits. We discovered that the mean number of profile visits is roughly at the 75th percentile. 25% of users tend to have extremely large numbers of profile visits.

The median number of unique user accounts that “liked” a user’s account is 44, with the number of likes ranging from 0 to 9288. Again, we saw that the median was close to the 75th percentile and there were several users with a large number of likes.

The median distance between this user's city/location and the location of the user account that was used to fetch the data is 173, with the range being from 0 to 6918. No units of measurement are indicated in the dataset.

The users are from 32 different countries ranging from 1 user to 1657 users with Italy, France, Germany and Switzerland being the countries with the most users. As mentioned previously 28 of the 32 countries have 20 or less users with 13 of these only having 1 user. As a result we decided to group all countries that had 20 users or less into one new category called “other”. This gave us 5 countries – Italy with 138 users, France with 646 users, Germany with 1468 users, Switzerland with 1657 users and Other with 83 users.

Of the 3992 users 91 of them have purchased VIP leaving 3901 users that have not purchased VIP.

From these results we can see that our data are not normally distributed, heavily skewed and had several outliers that were influencing our results. You can see an example of the original dataset for the density relating to the number of profile pictures users have in figure \@ref(fig:picturesplot).

```{r, echo=FALSE}
# Reading in the data
lovoo <- read.csv("../../Data/CSV/lovoo_v3_users_api-results.csv")

# Converting data to a data frame
lovoo <- data.frame(lovoo) 

# Selecting the columns that contain our variables
lovoo <- lovoo[, c("age", "counts_pictures", "counts_profileVisits", 
                   "counts_kisses", "distance", "country", "isVip")]

# Replacing the missing values in distance with the mean 207.23
lovoo$distance[is.na(lovoo$distance)] <- 207.23

# Replacing variable names
lovoo["country"][lovoo["country"] == 'CH'] <- 'Switzerland'
lovoo["country"][lovoo["country"] == 'DE'] <- 'Germany'
lovoo["country"][lovoo["country"] == 'FR'] <- 'France'
lovoo["country"][lovoo["country"] == 'IT'] <- 'Italy'
lovoo["country"][lovoo["country"] == 'AR'] <- 'Other'
lovoo["country"][lovoo["country"] == 'AT'] <- 'Other'
lovoo["country"][lovoo["country"] == 'AU'] <- 'Other'
lovoo["country"][lovoo["country"] == 'BA'] <- 'Other'
lovoo["country"][lovoo["country"] == 'BE'] <- 'Other'
lovoo["country"][lovoo["country"] == 'BR'] <- 'Other'
lovoo["country"][lovoo["country"] == 'CA'] <- 'Other'
lovoo["country"][lovoo["country"] == 'CF'] <- 'Other'
lovoo["country"][lovoo["country"] == 'CZ'] <- 'Other'
lovoo["country"][lovoo["country"] == 'ES'] <- 'Other'
lovoo["country"][lovoo["country"] == 'ET'] <- 'Other'
lovoo["country"][lovoo["country"] == 'GB'] <- 'Other'
lovoo["country"][lovoo["country"] == 'HU'] <- 'Other'
lovoo["country"][lovoo["country"] == 'ID'] <- 'Other'
lovoo["country"][lovoo["country"] == 'IN'] <- 'Other'
lovoo["country"][lovoo["country"] == 'JM'] <- 'Other'
lovoo["country"][lovoo["country"] == 'LI'] <- 'Other'
lovoo["country"][lovoo["country"] == 'LR'] <- 'Other'
lovoo["country"][lovoo["country"] == 'LU'] <- 'Other'
lovoo["country"][lovoo["country"] == 'NL'] <- 'Other'
lovoo["country"][lovoo["country"] == 'PE'] <- 'Other'
lovoo["country"][lovoo["country"] == 'PH'] <- 'Other'
lovoo["country"][lovoo["country"] == 'RO'] <- 'Other'
lovoo["country"][lovoo["country"] == 'RU'] <- 'Other'
lovoo["country"][lovoo["country"] == 'SC'] <- 'Other'
lovoo["country"][lovoo["country"] == 'TR'] <- 'Other'
lovoo["country"][lovoo["country"] == 'UA'] <- 'Other'
lovoo["country"][lovoo["country"] == 'US'] <- 'Other'

# Setting country and isVip to be factors.
lovoo$country <- as.factor(lovoo$country)
lovoo$isVip <- as.factor(lovoo$isVip)
```


```{r picturesplot, fig.cap="Profile pictures", echo=FALSE}
picsplot <- ggplot(lovoo, aes(x=counts_pictures)) +
  geom_histogram(aes(y=..density..),
                 bins=nclass.FD(unlist(lovoo$counts_pictures)),
                 col='black', fill='white') +
  labs(x = 'Number of Profile Pictures',
       y = 'Histogram, smoothed histogram, and estimated normal density')+
  ggtitle("Visualisation of the number of profile pictures a user has") +
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_density(col='blue', size=1.5) +
  geom_boxplot(aes(y=-0.02), width = .01, notch=TRUE, col='blue') +
  stat_function(fun = dnorm, args = list(mean = mean(lovoo$counts_pictures), 
                                         sd = sd(lovoo$counts_pictures)), 
                col='darkgreen', size=1.5) +
  geom_vline(xintercept = mean(lovoo$counts_pictures), col="red", size=1) +
  theme_tufte()

picsplot + theme(text=element_text(size=12, 
        family="serif")) +
  theme(plot.title=element_text(hjust=0.5))
```

Our EDA then looked at bivariate and multivariate analysis in varying combinations of the seven variables. 

We found that there is a strong positive correlation (0.89) between the number of profiles visits and the number of likes that a user receives. There is also a positive correlation (0.42) between the number of pictures a user has and the number of profile visits they receive and a positive correlation (0.37) between the number of pictures a user has and the number of likes they receive. There is a slight positive correlation (0.13) between the age of the user and the distance between this user’s city/location and the location of the user account that was used to fetch the data of the user. There appears to be no correlation between age and likes, profile visits and pictures, nor distance and likes, profile visits and pictures. This can be seen in the heatmap in figure \@ref(fig:heatmap).

```{r heatmap, fig.cap='Correlation heatmap', echo=FALSE}
cormat <- round(cor(lovoo[,1:5]),2)
melted_cormat <- melt(cormat)
ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()+ 
geom_text(aes(Var2, Var1, label = value), color = "black", size = 2.5)
```

In the univariate analysis we saw that 3901 users are not Vip’s while only 91 are Vip’s. When comparing VIP status with country we saw that Switzerland has the largest number of users with VIP status.

When looking at the number of users by age and country we saw that the number of users by age tends to increase in Germany and peak at 22 years old before decreasing. Whereas in France and Switzerland they tend to start with a larger number of users at a younger age and decrease as age increases.

# Cluster Analysis

To help us answer our question, we’ve used cluster analysis to classify groups of observations that get lots of likes and those that don’t using k-means and to see which numerical variables get the best results. For this we used the normalised and outlier removed dataset.

From the skree plot (figure \@ref(fig:skreeplot)) we can see that the optimal number of clusters is 2. 

```{r, echo=FALSE}
lovoo <- read.csv("../../Data/CSV/lovoo_v3_users_api-results.csv")
lovoo <- data.frame(lovoo)

lovoo <- lovoo[, c("age", "counts_pictures", "counts_profileVisits", 
                   "counts_kisses", "distance", "country", "isVip")]
# Replacing the missing values in distance with the mean 207.23
lovoo$distance[is.na(lovoo$distance)] <- 207.23

lovoo_standard <- as.data.frame(scale(lovoo[2:5]))


colnames(lovoo_standard) <- c("standard_pics", "standard_visits", "standard_kisses",
                              "standard_distance")

#head(lovoo)

z_scores <- as.data.frame(sapply(lovoo_standard[1:4], function(lovoo_standard) (abs(lovoo_standard-mean(lovoo_standard))/sd(lovoo_standard))))

z_scores$isVip <- lovoo$isVip
z_scores$country <- lovoo$country
z_scores$age <- lovoo$age

z_scores$isVip <- as.factor(z_scores$isVip)
z_scores$country <- as.factor(z_scores$country)
z_scores$age <- as.factor(z_scores$age)

colnames(z_scores) <- c("standard_pics", "standard_visits", "standard_kisses",
                        "standard_distance", "isVip", "country", 'age')

no_outliers <- z_scores[!rowSums(z_scores[1:4]>3), ]

#no_outliers$you_get_rizz <- with(no_outliers, ifelse(standard_kisses
#0.4, 1, 0))

#no_outliers$you_get_rizz <- as.factor(no_outliers$you_get_rizz)

#head(no_outliers)

result <- kmeans(no_outliers[1:4],
                 algorithm="MacQueen",
                 centers=2, iter.max = 100, nstart = 50)
#print(result)

```

```{r skreeplot, fig.cap="Skree Plot", echo=FALSE}

fviz_nbclust(no_outliers[1:4], kmeans, method = "wss")
```

The Cluster plot (figure \@ref(fig:caplot)) shows the split of the clusters.

```{r caplot, fig.cap="Cluster plot", echo=FALSE}
fviz_cluster(result, data = no_outliers[1:4], 
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw()
)

final_data <- cbind(no_outliers, cluster = result$cluster)
#head(final_data)
```

We calculated the standardised mean of each numerical variable in each cluster. We find a clear difference in the number of likes between clusters. Observations in cluster 2 on average get more likes than observations in cluster 1. Observations in cluster 2 on average have more visits than observations in cluster 1, but this is obvious as likes and visits are highly correlated. We see that cluster 2 has on average observations with more pictures and from a shorter distance. This can be seen in table \@ref(catab)

```{r catab, results = 'asis', echo=FALSE, message=FALSE, warning=FALSE}
caresult <- data.frame(aggregate(no_outliers[1:4], by=list(cluster=result$cluster), mean))
print(xtable(caresult, caption='Cluster Analysis Output', label='catab'), 
             caption.placement="top", type="latex", comment=FALSE)
```

# Linear discriminant analysis

## Methodology

We used linear discriminant analysis on the standardised and outlier removed data including all variables. We wanted to see if we would be able to predict whether someone would have a high chance of finding a match on the dating application, Lovoo. We also grouped the observations into whether they had a normal chance of a match or a very low chance, giving us three chance groups total. We used the count of kisses (likes) as the indicator for the groups; higher than the third quantile (0.4) is classed as highly likely, and lower than the first quantile (0.23) is classed as unlikely. There is an uneven proportion seen between the three classes as seen in the lower right of the group-coloured pairs plot (figure \@ref(fig:LDApairs)). We used the count of kisses as our group indication as we believe that the more likes you get on your profile can equate to more matches, therefore giving the profile user a higher chance of a successful dating app outcome (i.e., a relationship). After creating these groups, we removed the count of kisses from our analysis and classification attempt.

Looking at the pairs plot again (figure \@ref(fig:LDApairs)) we can see that there is some separation between groups that we can see visually (especially the highly likely class breaking away from the rest in the count of visits) but not much. In the univariate plots we can see that the data is not all normally distributed even after standardisation. Please note that country has been reduced to three levels instead of five for easy visualisation and analysis.

```{r createData, echo=FALSE}
lovoo <- read.csv("../../Data/CSV/lovoo_v3_users_api-results.csv")
lovoo <- data.frame(lovoo)

lovoo <- lovoo[, c("age", "counts_pictures", "counts_profileVisits", 
                   "counts_kisses", "distance", "country", "isVip")]
# Replacing the missing values in distance with the mean 207.23
lovoo$distance[is.na(lovoo$distance)] <- 207.23

lovoo_standard <- as.data.frame(scale(lovoo[2:5]))


colnames(lovoo_standard) <- c("standard_pics", "standard_visits", "standard_kisses",
                              "standard_distance")

z_scores <- as.data.frame(sapply(lovoo_standard[1:4], function(lovoo_standard) (abs(lovoo_standard-mean(lovoo_standard))/sd(lovoo_standard))))

z_scores$isVip <- lovoo$isVip
z_scores$country <- lovoo$country
z_scores$age <- lovoo$age

colnames(z_scores) <- c("standard_pics", "standard_visits", "standard_kisses",
                              "standard_distance", "isVip", "country", 'age')

no_outliers <- z_scores[!rowSums(z_scores[1:4]>3), ]

no_outliers["country"][no_outliers["country"] == 'CH'] <- 'Switzerland'
no_outliers["country"][no_outliers["country"] == 'DE'] <- 'Germany'

# Replacing variable names
no_outliers <- transform(no_outliers, 
                    country = {x <- as.character(country)
                          x[!x %in% c("Switzerland","Germany")] <- "other"
                          factor(x)})

no_outliers$country <- as.factor(no_outliers$country)
no_outliers$isVip <- as.factor(no_outliers$isVip)

q1<-quantile(no_outliers$standard_kisses, probs = c(0.25))
q3<-quantile(no_outliers$standard_kisses, probs = c(0.75))

binned <- no_outliers %>% mutate(Group =
                                   case_when(standard_kisses <= q1 ~ "Unlikely", 
                                             standard_kisses < q3 ~ "Normal",
                                             standard_kisses >= q3 ~ "Highly_likely")
)

binned$Group<-as.factor(binned$Group)

binned = subset(binned, select = -c(standard_kisses))
```

```{r LDApairs, message=FALSE, echo=FALSE, fig.cap="Chance group-coloured pairs plot for LDA"}
ggpairs(binned, aes(alpha=.5, color=Group)) +
  theme_pander() +
  theme(text=element_text(family="serif"))
```

## Results

When conducting the linear discriminant analysis, we used the total five measures. We split into test and train sets randomly with a split of 80/20. From the group means output in table \@ref(LDAmeans), we can see that photo count and visits count have the highest influence on our predetermined groups, this confirms what we discovered about visits being correlated with kisses and therefore our dating chance groups. There is also a difference seen in Switzerland, with that country showing that it has a higher influence on the unlikely dating chance group compared to the other two factors.

```{r LDAmodelOutput, echo=FALSE}
set.seed(300521444, kind="Mersenne-Twister")

ind <- sample(c("Tr", "Te"),
nrow(binned),
replace=TRUE,
prob=c(.8, .2))

# Explicitly split the data in "Train" and "Test"
Train <- binned[ind=="Tr",]
Test <- binned[ind=="Te",]
LDA <- lda(Group ~ .,
data=Train)
```

```{r ldat4, results = 'asis', echo=FALSE, message=FALSE, warning=FALSE}
LDAmeans<-LDA$means

row.names(LDAmeans) = str_wrap(row.names(LDAmeans), width=50)

print(xtable(LDAmeans, caption='Group means of Linear discriminate analysis', label='LDAmeans', align=c('p{1in}',rep('|c', ncol(LDAmeans)))),scalebox = 0.75, caption.placement="top", type="latex", comment=FALSE)

#print(xtable(LDAmeans, caption="Group means of Linear discriminate analysis", 
#             label="LDAmeans"), comment=FALSE, 
#            caption.placement="top", type="latex")
```

The scaling coefficients in table \@ref(LDAcoef) show that the first linear discriminant is largely explained by the feature count of visits (as expected). The proportion of trace (between-class variance) of the first linear discriminant is 94% and the second is 6%. This informs the separability of the training data into the three classes by each linear discriminant.

```{r ldat3, results = 'asis', echo=FALSE}
LDAcoef<-LDA$scaling
print(xtable(LDAcoef, caption="Scaling coeficients of Linear discriminate analysis", 
             label="LDAcoef"), comment=FALSE,
            caption.placement="top", type="latex")
```

When applying our analysis results using the predict function, we can plot the predicted score histograms. There was complete overlap in histograms between all groups in LD1 and LD2 meaning that neither of the discriminates could separate each group from each other, which is a bad outcome for our classification goal. Presented in figure \@ref(fig:LDAhist) is the predicted score histogram of LD1. There is a complete overlap of all the groups in the biplot presented in figure \@ref(fig:LDAbiplot). From this we have gone back and tested a reduced number of measures in the analysis to just count of photos, visits, and country. The results were similar so we continued to investigate the full model. Once again, we can see the large affect of visits on the model from the magnitude of the vector showing more discrimination between groups.

```{r LDAhist, echo=FALSE, fig.cap="Predicted score histograms for LD1"}
Pred <- predict(LDA)
ldahist(data=Pred$x[,1], g=Train$Group)
```

```{r LDAbiplot, echo=FALSE, fig.cap="Linear discriminant biplot"}
ggord(LDA, Train$Group)
```

## Linear discriminant analysis Conclusion

When analysing the predicted vs observed classification table (table \@ref(RCM)) using the test data we set aside earlier we can see that the reported accuracy using LDA as a classification technique is 56% (with the biased optimistic accuracy on the training set matching this score). The classifier over classifies the dating app users as being in the normal chance group, meaning that there might not be any predictors included in the model that can be defined as outstanding dating profile features worthy of more matches.

```{r ldat1, results = 'asis', echo=FALSE}
RealisticPredicted <- predict(LDA, Test)$class
RCM <- table(RealisticPredicted, Actual=Test$Group)

print(xtable(RCM, caption="Realistic predicted LDA classification", 
             label="RCM"), comment=FALSE,
            caption.placement="top", type="latex")
```

This poor result maybe be due to the data failing the Box's M-test for Homogeneity of Covariance Matrices with a p-value of $2.2x10^{-16}$ and the classes being imbalanced. From these results, we applied quadratic discriminant analysis as this technique works better for data that fails the Box' M test. We did not see an improvement in classification accuracy with QDA achieving the same score of 56% on the test set. The results can be seen in table \@ref(QDAtab).

```{r ldat2, results = 'asis', echo=FALSE}
qda <- qda(Group~., data = Train)
qda.predict <- caret::train(Group~., method = "qda", data = Train)
cm<-confusionMatrix(Test$Group, predict(qda.predict, Test))
QDAtab <- as.table(cm)

print(xtable(QDAtab, caption="Realistic predicted QDA classification", 
             label="QDAtab"), comment=FALSE,
            caption.placement="top", type="latex")
```
  
# Principal component analysis 
 
## Method
Much like in the linear discriminant analysis, we are going to try and predict dating chance groups. We have used the four numeric measures, count of visits, count of photos, distance and age and are going to use the same train/test split. 
  
As we can see in table \@ref(pcasum), two principal components only explain 56% of the variance and three explains 78%. This is visualised in figure \@ref(fig:pcvarplot). We will need to use three principal components for predictions as this explains the majority. 
  
We have plotted the pairs graph of the components in figure \@ref(fig:PCApairs). As seen in this plot, all correlation coefficients are equal to zero, all components are very normally distributed in the univariate plots, and there seems to be some funnelling in the bivariate plots. 
  
```{r echo=FALSE}
lovoo <- read.csv("../../Data/CSV/lovoo_v3_users_api-results.csv")
lovoo <- data.frame(lovoo)

lovoo <- lovoo[, c("age", "counts_pictures", "counts_profileVisits", 
                   "counts_kisses", "distance", "country", "isVip")]
# Replacing the missing values in distance with the mean 207.23
lovoo$distance[is.na(lovoo$distance)] <- 207.23

lovoo_standard <- as.data.frame(scale(lovoo[2:5]))


colnames(lovoo_standard) <- c("standard_pics", "standard_visits", "standard_kisses",
                              "standard_distance")

z_scores <- as.data.frame(sapply(lovoo_standard[1:4], function(lovoo_standard) (abs(lovoo_standard-mean(lovoo_standard))/sd(lovoo_standard))))

z_scores$isVip <- lovoo$isVip
z_scores$country <- lovoo$country
z_scores$age <- lovoo$age

colnames(z_scores) <- c("standard_pics", "standard_visits", "standard_kisses",
                              "standard_distance", "isVip", "country", 'age')

no_outliers <- z_scores[!rowSums(z_scores[1:4]>3), ]

no_outliers["country"][no_outliers["country"] == 'CH'] <- 'Switzerland'
no_outliers["country"][no_outliers["country"] == 'DE'] <- 'Germany'

# Replacing variable names
no_outliers <- transform(no_outliers, 
                    country = {x <- as.character(country)
                          x[!x %in% c("Switzerland","Germany")] <- "other"
                          factor(x)})

no_outliers$country <- as.factor(no_outliers$country)
no_outliers$isVip <- as.factor(no_outliers$isVip)

q1<-quantile(no_outliers$standard_kisses, probs = c(0.25))
q3<-quantile(no_outliers$standard_kisses, probs = c(0.75))

binned <- no_outliers %>% mutate(Group =
                                   case_when(standard_kisses <= q1 ~ "Unlikely", 
                                             standard_kisses < q3 ~ "Normal",
                                             standard_kisses >= q3 ~ "Highly_likely")
)

binned$Group<-as.factor(binned$Group)

binned = subset(binned, select = -c(standard_kisses, country, isVip))
```

```{r echo=FALSE}
set.seed(300521444, kind="Mersenne-Twister")

ind <- sample(c("Tr", "Te"),
nrow(no_outliers),
replace=TRUE,
prob=c(.8, .2))

# Explicitly split the data in "Train" and "Test"
Train <- binned[ind=="Tr",]
Test <- binned[ind=="Te",]
```

```{r pcat1, results = 'asis', echo=FALSE}
pc <- prcomp(Train[,-5],
             center = TRUE,
            scale. = TRUE)
pcasum<-summary(pc)

print(xtable(pcasum, caption="Summary of the principal component analysis", 
             label="pcasum"), comment=FALSE,
            caption.placement="top", type="latex")

```

```{r pcvarplot, echo=FALSE, fig.cap="The variance explained per principal component"}
plot(pc, type="l")
```

```{r PCApairs, echo=FALSE, fig.cap="Pairs plot of all principal components"}
pairs.panels(pc$x,
             gap=0,
             bg = c("red", "yellow", "blue")[Train$Group],
             pch=21)
```
  
## Result
  
The biplot of the first two principal components (figure \@ref(fig:bipca)) shows that 56% explained variance does not differentiate between the chance groups as there is complete overlap. The feature arrows indicate that each variable does not provide much discrimination. 
  
When applying this analysis to a classification problem we see worse results than the LDA classification attempt. The model we have created includes a multinomial logistic regression of the first three principal components with the unlikely chance group as the reference level. We have used this model as we are predicting more than two unordered categorical groups as our target. The residual deviance of the model is *5575.5*.
  
Table \@ref(tab1) shows the predicted verses actual results of applying the model to the test data. Again, we can see that the classifier over represents the normal chance group and most likely has to do with the class imbalance. The Accuracy of this technique is only 24%. 
  
```{r bipca, echo=FALSE, fig.cap="Biplot of the first two principal components"}
ggord(pc, Train$Group)
```

```{r echo=FALSE, include=FALSE}
set.seed(300521444, kind="Mersenne-Twister")
trg <- predict(pc, Train)
trg <- data.frame(trg, Train[5])
tst <- predict(pc, Test)
tst <- data.frame(tst, Test[5])
trg$Group <- relevel(trg$Group, ref = "Unlikely")
mymodel <- nnet::multinom(Group~PC1+PC2+PC3, data = trg)
#summary(mymodel)
```
```{r pcat2, results = 'asis', echo=FALSE}
p1 <- predict(mymodel, tst)
tab1 <- table(p1, tst$Group)

print(xtable(tab1, caption="Predicted vs Actual PCA classification", 
             label="tab1"), comment=FALSE,
            caption.placement="top", type="latex")

```
  
## Principal component analysis Conclusion
  
We might have achieved this bad result with PCA as it removes variability without knowledge of the features that are most useful in classification as it reduces dimensionality. It was still worthwhile trying this technique as it adds evidence to our final conclusion.


# Conclusion

Our methods of analysis have drawn us to the conclusion that more profile visits inherently mean more kisses (which is our measure of dating success). The number of photos on our subject’s profile, their country of origin, age, their distance away from potential matches, and whether they pay for a VIP subscription does not influence a successful dating outcome. 

With only one predictor having a substantial amount of influence on our target variable, we did not receive significant results from our classification attempts. 

Linear discriminant analysis and principal component analysis did not offer any robust results, but our cluster analysis did provide some insight into our leading question, concluding that a shorter distance and higher number of photos will separate you from the crowd in the dating world.

If we had more and/or different measures available to us, we may be able to deduce what features attract profile visits and, therefore, more kisses with a higher level of confidence. With the data we have access to as of now, we can only conclude that love is unpredictable. 

# Contact details

| Co-Authors      | Contact details      |
| --------------- | ------------- |
| \newline ![]("../../Images/JPG/lissa-harrop.jpg"){height=120px} \newline | \newline Lissa Harrop \newline harropliss@myvuw.ac.nz \newline \href{https://orcid.org/0000-0002-6761-7465}{\color{blue}ORCID: 0000-0002-6761-7465} |
| \newline ![]("../../Images/JPG/Ricky.jpg"){height=120px} | \newline Ricky Loo \newline loorick@myvuw.ac.nz \newline \href{https://orcid.org/0000-0002-4265-1085}{\color{blue}ORCID: 0000-0002-4265-1085} | 
| \newline ![]("../../Images/JPG/Max_image.jpg"){height=120px} | \newline Max Tan \newline tanmacs@myvuw.ac.nz \newline \href{https://orcid.org/0000-0001-9215-4311}{\color{blue}ORCID: 0000-0001-9215-4311} | 
| \newline ![]("../../Images/PNG/katrina_image2.png"){height=120px} | \newline Katrina Watkins\newline watkinkatr@myvuw.ac.nz \newline \href{https://orcid.org/0000-0001-6614-2819}{\color{blue}ORCID: 0000-0001-6614-2819} | 

# References

